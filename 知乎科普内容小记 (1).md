# 知乎科普内容小记
感知机（perception）：输入层、输出层、隐含层

 - 输入的特征向量通过隐含层变换达到输出层，在输出层得到分类结果
 - 单层感知机
     - 只能处理线性可分问题,连异或都不能拟合
     - 线性可分：一个数据集中的所有样本可以被一个线性决策边界(线性分类器，如直线、平面或超平面）完美地划分为两类或多类，那么这个数据集就是线性可分
 - 多层感知机（神经网络NN neural network）：含有多个隐含层,使用连续函数模拟神经元对激励的响应，在训练算法上使用反向传播BP算法
     - 解决了之前无法模拟异或逻辑的缺陷，同时更多的层数也让网络更能够刻画现实世界中的复杂情形，神经网络的层数直接决定了它对现实的刻画能力
     - 随着神经网络层数的加深，优化函数越来越容易陷入局部最优解，“梯度消失”现象更加严重，层数一多，梯度指数衰减后低层基本上接受不到有效的训练信号，利用预训练方法缓解了局部最优解问题

DNN（深度神经网络 Deep Neural Network）

 - “深度”并没有固定的定义——在语音识别中4层网络就能够被认为是“较深的”，而在图像识别中20层以上的网络屡见不鲜
 - 单从结构上来说，全连接的DNN和图1的多层感知机是没有任何区别的。
 - 全连接DNN的潜在问题
     - 参数数量的膨胀,不仅容易过拟合，而且极容易陷入局部最优
     - 无法对时间序列上的变化进行建模

CNN（卷积神经网络 Convolutional Neural Network）
 - 并不是所有上下层神经元都能直接相连，而是通过“卷积核”作为中介，同一个卷积核在所有图像内是共享的，图像通过卷积操作后仍然保留原先的位置关系
 - 适用于图像识别，正是由于CNN模型限制参数的个数并挖掘了局部结构的这个特点，利用语音语谱结构中的局部信息，CNN照样能应用在语音识别中

RNN（循环神经网络 Recurrent Neural Network）
 - 在RNN中，神经元的输出可以在下一个时间戳直接作用到自身，即第i层神经元在m时刻的输入，除了（i-1）层神经元在该时刻的输出外，还包括其自身在（m-1）时刻的输出！
 - RNN可以看成一个在时间上传递的神经网络，它的深度是时间的长度，“梯度消失”现象又要出现了，，只不过这次发生在时间轴上，根本就无法影响太遥远的过去
 - 为了解决时间上的梯度消失，机器学习领域发展出了长短时记忆单元LSTM，通过门的开关实现时间上记忆功能，并防止梯度消失
 - 在序列信号分析中，如果我能预知未来，对识别一定也是有所帮助的，因此就有了双向RNN、双向LSTM，同时利用历史和未来的信息

不论是那种网络，他们在实际应用中常常都混合着使用，比如CNN和RNN在上层输出之前往往会接上全连接层，很难说某个网络到底属于哪个类别










