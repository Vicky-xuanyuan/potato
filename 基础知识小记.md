# 基础知识小记
**梯度求导**
 - 计算一个函数在某个点的导数（梯度）的过程
 - 损失函数：损失函数衡量模型的预测与实际结果之间的差异。例如，在回归问题中，损失函数可以是均方误差（MSE）；在分类问题中，损失函数可以是交叉熵（cross-entropy）。
 - 梯度：梯度是损失函数对模型参数（例如权重和偏置）的偏导数。
 - 梯度下降（Gradient Descent）：一旦我们计算出了梯度，我们就可以通过梯度下降来更新模型的参数。梯度下降的基本步骤是：沿着梯度的反方向调整参数
 - **为什么梯度求导重要？**
    - **优化**：通过梯度求导，算法能够知道如何调整参数，逐渐逼近最优解。
    - **训练深度神经网络**：深度神经网络的训练依赖于梯度求导，尤其是在反向传播算法中，它通过计算每一层参数的梯度来优化网络的权重。
    - **提高精度**：有效的梯度计算可以帮助模型更快速、更准确地收敛，进而提高预测精度。

**BP算法（Backpropagation）**
 - **前向传播（Forward Propagation）**
   - 输入数据经过各层神经元计算，最终输出预测值。
   - 每一层的输出是下一层的输入。

- **计算损失（Loss Calculation）**
   - 将预测值与真实值比较，计算误差（例如使用均方误差或交叉熵损失函数）。

 - **反向传播（Backward Propagation）**
   - 使用链式法则，从输出层向输入层**逐层计算误差对权重的导数（梯度）**。
   - 利用这些梯度调整每一层的权重和偏置。

 - **更新参数（Gradient Descent）**
   - 使用梯度下降法，根据计算出的梯度更新神经网络中的参数，使得损失函数逐渐减小。

**词向量（Word Embedding）**
 - **基本概念** 
 - 词向量是一种将文字（词语）转换为向量（数字数组）的技术。它的核心思想是：把“词”表示成计算机能理解的数字形式，而且这种表示方式要能反映出词语之间的语义关系。
 - 词向量的目标：相似的词向量，表示语义上相似的词语
 - **常见的词向量方法**
     - One-hot 编码（最原始的方式）：每个词用一个长向量表示，只有一个位置是 1，其他都是 0。
     - Word2Vec（经典方法）：基于上下文训练词向量，让相似的词在向量空间中更近。
     - 两种模型：
        - CBOW（连续词袋模型）：预测中间词。
        - Skip-Gram：根据中间词预测周围词。
    - GloVe（全局向量）
    - FastText
    - BERT、GPT 等语言模型中的词向量

**语言模型**
- 语言模型是一个能够理解并生成自然语言的模型。根据已有的词或句子，预测下一个最可能出现的词

**卷积神经网络（CNN）**
 - 一种非常重要的深度学习模型，用于处理具有类似网格结构的数据，比如图像、语音和视频等
 - 输入层（Input Layer）
 - 卷积层（Convolutional Layer）:使用多个**卷积核（filter）**在图像上滑动（类似扫描），提取出图像中的特征，比如边缘、纹理等
 - 激活函数层（Activation Layer）:通常使用 ReLU（Rectified Linear Unit），引入非线性因素，让网络能学习更复杂的模式。
 - 池化层（Pooling Layer）：对卷积后的结果进行降采样（比如最大池化 max pooling），减少计算量和参数数量，同时提取更稳定的特征
 - 全连接层（Fully Connected Layer）
 - 输出层（Output Layer）

**循环神经网络**
 - 一种专门处理序列数据的神经网络，适合处理时间相关或上下文相关的问题（语音识别、股票预测、机器翻译（语句有上下文））
 - 传统神经网络（如全连接网络）是**“一次性”处理所有输入**，它不会记住前面的信息。而RNN会把上一步的输出结果作为“状态”，和当前输入一起传到下一步，实现信息的传递与记忆。
 - 问题：梯度消失 / 爆炸,长期依赖问题（很难记住太远的内容）

**attention机制**
- Attention（注意力机制）**是一种让模型能够“关注”输入中最重要部分的技术。对于每个输出，它不再只依赖一个输入位置，而是综合所有输入的信息，并给予“不同的权重”
- 每一个词都会被转换成三个向量：
    - **Query（查询）**
    - **Key（键）**
    - **Value（值）**
    - 最终的输出是对所有输入的加权平均，权重就是“注意力”。

**Transformer**
- Transformer 是一种基于 Attention 机制的神经网络模型，是一种专门用来处理序列数据（特别是语言）的深度学习模型，是 RNN 的“升级版”
- Transformer 完全摒弃了循环结构，全部使用 Attention，可以并行、高效、准确地处理长文本。
- Transformer 模型由编码器、解码器组成
    - 编码器（Encoder）：把输入（比如英文句子）变成隐藏的表示（向量），每个词都能“看到”其他所有词（通过 **Self-Attention**）
    - 解码器（Decoder）：生成输出（比如翻译成中文），每一步生成下一个词，也能通过 Attention 看 Encoder 的内容

**BERT**
 - BERT（Bidirectional Encoder Representations from Transformers）：一个预训练语言模型，它是基于 Transformer的Encoder部分构建 
 - 核心特点：“双向理解”文本上下文，拥有更深的语义理解能力。
 - BERT 的核心技术：
     - **双向 Transformer 编码器**：和 GPT 不同（GPT 是单向），BERT 使用 Masked Language Model，从一开始就训练模型能理解两边的信息。
     - 两种预训练任务：
        - MLM（Masked Language Model）：随机遮掉句子中的词让模型猜出被遮的词；
        - NSP（Next Sentence Prediction）给两个句子，判断第二句是不是第一句的下文

**GPT（Generative Pre-trained Transformer）**
 - GPT（生成式预训练 Transformer）核心思路是：用大量文本 预训练 一个强大的 Transformer Decoder，然后根据输入自动生成文本。
 - “语言生成+理解+推理”能力的通用 AI 模型，靠 Transformer 架构、海量数据和人类反馈训练
 - 工作流程
    - 预训练（Pretraining）用海量数据训练模型，让它学会语言的结构和逻辑。目标是：根据前面的词预测下一个词（这是语言模型的核心）
    - 微调 / 指令调教（Fine-tuning / Instruction tuning）：在特定任务上微调，或者用人类反馈（RLHF）来优化输出的质量和安全性
    - 生成（Generation）
 - GPT 是 基于 Transformer Decoder 的模型，和 BERT（用的是 Encoder）不同。
    - BERT	Encoder	理解文本（双向）
    - GPT	Decoder	生成文本（单向）
- 优势：
    - 统一模型，通吃任务，不需要为每个任务训练一个新模型，
    - 少样本/零样本学习（few-shot / zero-shot），
    - 可以通过“提示词”（Prompt）引导它完成任务

