# 基础知识小记
**梯度求导**
 - 计算一个函数在某个点的导数（梯度）的过程
 - 损失函数：损失函数衡量模型的预测与实际结果之间的差异。例如，在回归问题中，损失函数可以是均方误差（MSE）；在分类问题中，损失函数可以是交叉熵（cross-entropy）。
 - 梯度：梯度是损失函数对模型参数（例如权重和偏置）的偏导数。
 - 梯度下降（Gradient Descent）：一旦我们计算出了梯度，我们就可以通过梯度下降来更新模型的参数。梯度下降的基本步骤是：沿着梯度的反方向调整参数
     - 更新规则通常是：
   \[
   \theta = \theta - \eta \nabla_{\theta} L(\theta)
   \]
   其中，\(\theta\) 是模型的参数，\(\eta\) 是学习率，\(\nabla_{\theta} L(\theta)\) 是参数\(\theta\)对应的梯度，\(L(\theta)\) 是损失函数。
 - **为什么梯度求导重要？**
    - **优化**：通过梯度求导，算法能够知道如何调整参数，逐渐逼近最优解。
    - **训练深度神经网络**：深度神经网络的训练依赖于梯度求导，尤其是在反向传播算法中，它通过计算每一层参数的梯度来优化网络的权重。
    - **提高精度**：有效的梯度计算可以帮助模型更快速、更准确地收敛，进而提高预测精度。

**BP算法（Backpropagation）**
 - **前向传播（Forward Propagation）**
   - 输入数据经过各层神经元计算，最终输出预测值。
   - 每一层的输出是下一层的输入。

- **计算损失（Loss Calculation）**
   - 将预测值与真实值比较，计算误差（例如使用均方误差或交叉熵损失函数）。

 - **反向传播（Backward Propagation）**
   - 使用链式法则，从输出层向输入层**逐层计算误差对权重的导数（梯度）**。
   - 利用这些梯度调整每一层的权重和偏置。

 - **更新参数（Gradient Descent）**
   - 使用梯度下降法，根据计算出的梯度更新神经网络中的参数，使得损失函数逐渐减小。

**词向量（Word Embedding）**
 - **基本概念** 
 - 词向量是一种将文字（词语）转换为向量（数字数组）的技术。它的核心思想是：把“词”表示成计算机能理解的数字形式，而且这种表示方式要能反映出词语之间的语义关系。
 - 词向量的目标：相似的词向量，表示语义上相似的词语
 - **常见的词向量方法**
     - One-hot 编码（最原始的方式）：每个词用一个长向量表示，只有一个位置是 1，其他都是 0。
     - Word2Vec（经典方法）：基于上下文训练词向量，让相似的词在向量空间中更近。
     - 两种模型：
        - CBOW（连续词袋模型）：预测中间词。
        - Skip-Gram：根据中间词预测周围词。
    - GloVe（全局向量）
    - FastText
    - BERT、GPT 等语言模型中的词向量

**语言模型**
- 语言模型是一个能够理解并生成自然语言的模型。根据已有的词或句子，预测下一个最可能出现的词
**卷积神经网络**
**循环神经网络**
**attention机制**
- Attention（注意力机制）**是一种让模型能够“关注”输入中最重要部分的技术。对于每个输出，它不再只依赖一个输入位置，而是综合所有输入的信息，并给予“不同的权重”
- 每一个词都会被转换成三个向量：
    - **Query（查询）**
    - **Key（键）**
    - **Value（值）**
    然后通过下列公式计算注意力权重：
    \[
    \text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^\top}{\sqrt{d_k}} \right)V
    \]
    - \( Q \)：表示当前词在“找信息”
    - \( K \)：表示其他词的“内容标签”
    - \( V \)：实际的信息内容
    - \( d_k \)：向量维度（用于归一化）

    - 最终的输出是对所有输入的加权平均，权重就是“注意力”。

**Transformer**
- Transformer 是一种基于 Attention 机制的神经网络模型，专为处理序列数据设计。
- Transformer 完全摒弃了循环结构，全部使用 Attention，可以并行、高效、准确地处理长文本。
- Transformer 模型由编码器、解码器组成

**BERT**
**GPT**
，，，，，，，
